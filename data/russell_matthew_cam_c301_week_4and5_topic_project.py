# -*- coding: utf-8 -*-
"""Russell_Matthew_CAM_C301_Week_4and5_Topic_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OvIIQizYW80GT4HE_zBcZ6YYgGcgq_iy

**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.
Remember to update the portfolio index link to your own work once completed!

# Topic project 4.1 Applying NLP for topic modelling in a real-life context

**Welcome to your topic project: Applying NLP for topic modelling in a real-life context**

In this project, you will bridge the gap between theory and practical application by developing automated topic modelling tools tailored to a specific industry context.


Applying NLP for topic modelling is crucial for data analysis in business because it enables companies to identify and understand key themes and patterns within large volumes of text data. This efficiency allows businesses to extract essential insights and trends without manually sifting through extensive documents. Automated topic modelling helps businesses make informed decisions faster, which helps to improve productivity and gain a competitive edge. Additionally, it supports better information management by uncovering underlying topics in reports, emails, customer feedback, and market research, which enhances overall business intelligence and strategic planning.




Please set aside approximately **19 hours** to complete the topic project by **Thursday, 22 August at 5 p.m. (UK Time)**.

<br>

## **Business context**

The PureGym Group, founded in 2008, has approximately 2 million members and 600 gyms across the world (particularly in the UK, Denmark, and Switzerland). As one of the world’s largest value fitness operators, PureGym appeals to a broad range of customers by offering high-quality, low-cost, and flexible fitness facilities. The company’s customer-centric proposition – affordable membership fees, no fixed-term contracts, and 24/7 access to high-quality gyms – differentiates it from more traditional gyms and elevates it as a market leader within this space.

This focus on the customer is centred on wanting to understand what motivates members to join and what factors influence their behaviours once they have joined. Understanding how to leverage innovative technology to influence, improve, and simplify their experience allows PureGym to foster an open, welcoming, and diverse environment for its members while maintaining the value proposition that PureGym is built upon.

With the shift in focus to value-for-money memberships across the gym industry, PureGym seeks to achieve its mission of ‘inspiring a healthier world by providing members with affordable access to the benefits being healthy can offer’.


<br></br>

## **Objective**

By the end of this topic project, you will have analysed PureGym's review data to uncover key drivers that provide actionable insights for enhancing customer experience.

In the Notebook, you will:

- Use two data sets containing customer reviews from Google and Trustpilot.
- Perform basic level analysis by finding the frequently used words in both data sets.
- Generate a wordcloud to visualise the most frequently used words in the reviews.
- Apply BERTopic for topic modelling, keeping track of gym locations, to identify common topics and words in the negative reviews.
- Identify the locations that have the most negative reviews.
- Use the built-in visualisation functions in BERTopic to cluster and visually represent the topics and words in these reviews, thereby helping to identify specific themes from the reviews.
- Conduct a comparison with Gensim’s LDA model to validate the topic modelling results.
- Perform emotion analysis to identify the emotions associated with customer reviews.
- Filter out angry reviews and apply BERTopic to discover prevalent topics and words being discussed these negative reviews.
- Leverage the multi-purpose capability of the state-of-the-art Falcon-7b-instruct model, with the help of prompts, to identify top topics in each review.
- Use a different prompt with the Falcon-7b-instruct model to further generate suggestions for improvements for PureGym, based on the top topics identified from the negative reviews.


You will also write a report summarising the results of your findings and recommendations.


<br></br>

## **Assessment criteria**
By completing this project, you will be able to provide evidence that you can:

- Investigate real-world data to find potential trends for deeper investigation.
-Preprocess and refine textual data for visualisations.
-Apply topic modelling using various techniques.
-Apply emotion analysis using BERT.
-Evaluate the outcomes of your investigation.
-Communicate actionable insights.



<br></br>

## **Project guidance**

**Import packages and data:**
1. 1. Import the data set with the provided URL:
  - Data set's Drive link: https://drive.google.com/drive/folders/1azch13dtGeAkbnEjqeoyl3nU9jheojU4
2. Import the data file **Google_12_months.xlsx** into a dataframe.
3. Import the data file **Trustpilot_12_months.xslx** into a dataframe.
4. Remove any rows with missing values in the Comment column (Google review) and Review Content column (Trustpilot).


**Conducting initial data investigation:**

1. - Find the number of unique locations in the Google data set.
  
  - Find the number of unique locations in the Trustpilot data set.
  
  - Use Club's Name for the Google data set.
  
  - Use Location Name for the Trustpilot data set.
2. Find the number of common locations between the Google data set and the Trustpilot data set.
3. Perform preprocessing of the data – change to lower case, remove stopwords using NLTK, and remove numbers.
4. Tokenise the data using word_tokenize from NLTK.
5. Find the frequency distribution of the words from each data set's reviews separately. You can use nltk.freqDist.
6. Plot a histogram/bar plot showing the top 10 words from each data set.
7. Use the wordcloud library on the cleaned data and plot the word cloud.
8. Create a new dataframe by filtering out the data to extract only the negative reviews from both data sets.

  • For Google reviews, overall scores < 3 can be considered negative scores.

  • For Trustpilot reviews, stars < 3 can be considered negative scores.

  Repeat the frequency distribution and wordcloud steps on the filtered data consisting of only negative reviews.


**Conducting initial topic modelling:**

1. With the data frame created in the previous step:

  • Filter out the reviews that are from the locations common to both data sets.

  • Merge the reviews to form a new list.

2. Preprocess this data set. Use BERTopic on this cleaned data set.
3. Output: List out the top topics along with their document frequencies.
4. For the top 2 topics, list out the top words.
5. Show an interactive visualisation of the topics to identify the cluster of topics and to understand the intertopic distance map.
6. Show a barchart of the topics, displaying the top 5 words in each topic.
7. Plot a heatmap, showcasing the similarity matrix.
8. For 10 clusters, provide a brief description in the Notebook of the topics they comprise of along with the general theme of the cluster, evidenced by the top words within each cluster's topics.

**Performing further data investigation:**

1. List out the top 20 locations with the highest number of negative reviews. Do this separately for Google and Trustpilot's reviews, and comment on the result. Are the locations roughly similar in both data sets?
2. Merge the 2 data sets using Location Name and Club's Name.

  Now, list out the following:

  • Locations

  • Number of Trustpilot reviews for this location

  • Number of Google reviews for this location

  • Total number of reviews for this location (sum of Google reviews and Trustpilot reviews)

  Sort based on the total number of reviews.
3. For the top 30 locations, redo the word frequency and word cloud. Comment on the results, and highlight if the results are different from the first run.
4. For the top 30 locations, combine the reviews from Google and Trustpilot and run them through BERTopic.

  Comment on the following:

  • Are the results any different from the first run of BERTopic?

  • If so, what has changed?

  • Are there any additional insights compared to the first run?

**Conducting emotion analysis:**

1. Import the BERT model bhadresh-savani/bert-base-uncased-emotion from Hugging Face, and set up a pipeline for text classification.
2. With the help of an example sentence, run the model and display the different emotion classifications that the model outputs.
3. Run this model on both data sets, and capture the top emotion for each review.
4. Use a bar plot to show the top emotion distribution for all negative reviews in both data sets.
5. Extract all the negative reviews (from both data sets) where anger is top emotion.
6. Run BERTopic on the output of the previous step.
7. Visualise the clusters from this run. Comment on whether it is any different from the previous runs, and whether it is possible to narrow down the primary issues that have led to an angry review.

**Using a large language model from Hugging Face:**

1. Load the following model: tiiuae/falcon-7b-instruct. Set the pipeline for text generation and a max length of 1,000 for each review.
2. Add the following prompt to every review, before passing it on to the model: **In the following customer review, pick out the main 3 topics. Return them in a numbered list format, with each one on a new line.**

  Run the model.

  Note: If the execution time is too high, you can use a subset of the bad reviews (instead of the full set) to run this model.
3. The output of the model will be the top 3 topics from each review. Append each of these topics from each review to create a comprehensive list.
4. Use this list as input to run BERTopic again.
5. Comment about the output of BERTopic. Highlight any changes, improvements, and if any further insights have been obtained.
6. Use the comprehensive list from Step 3.

  Pass it to the model as the input, but pre-fix the following to the prompt: **For the following text topics obtained from negative customer reviews, can you give some actionable insights that would help this gym company?**

  Run the Falcon-7b-Instruct model.
7. List the output, ideally in the form of suggestions, that the company can employ to address customer concerns.

**Using Gensim:**
1. Perform the preprocessing required to run the LDA model from Gensim. Use the list of negative reviews (combined Google and Trustpilot reviews).
2. Using Gensim, perform LDA on the tokenised data. Specify the number of topics = 10.
3. Show the visualisations of the topics, displaying the distance maps and the bar chart listing out the most salient terms.
4. Comment on the output and whether it is similar to other techniques, or whether any extra insights were obtained.

**Report:**
1. Document your approach and major inferences from the data analysis.
2. When you have completed the project:
  - Download your completed Notebook as an IPYNB (Jupyter Notebook) or PY (Python) file. Save the file as follows: **LastName_FirstName_CAM_C301_Week_4and5_Topic_project.ipynb**.
  - Prepare a detailed report (between 800–1,000 words) that includes:
    - an overview of your approach
    - a description of your analysis
    - an explanation of the insights you identified
    - a summary of the comments requested in earlier steps
    - final insights, based on the output obtained from the various models employed
  - Save the document as a PDF named according to the following convention: **LastName_FirstName_CAM_C301_Week_4and5_Topic_project.pdf**.
  - Submit your Notebook and PDF document by **Thursday, 22 August at 5 p.m. (UK Time)**.


<br></br>
> **Declaration**
>
> By submitting your project, you indicate that the work is your own and has been created with academic integrity. Refer to the Cambridge plagiarism regulations.

# Importing Packages and Data
"""

!pip install contractions
!pip install bertopic

!pip uninstall -y transformers accelerate sentence-transformers bertopic
!pip install transformers accelerate sentence-transformers bertopic

# !pip install --upgrade --force-reinstall numpy
# !pip install --upgrade --force-reinstall scipy
# !pip install --upgrade --force-reinstall gensim

#Import the data set with the provided URL:
#Data set's Drive link: https://drive.google.com/drive/folders/1azch13dtGeAkbnEjqeoyl3nU9jheojU4
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import nltk
import string
from bs4 import BeautifulSoup
nltk.download('all')


goog_reviews = pd.read_excel('https://docs.google.com/spreadsheets/d/13TeZTT6oqk3iPELBhFx9hComIfChKw4m/export?format=xlsx', engine='openpyxl')
tp_reviews = pd.read_excel('https://docs.google.com/spreadsheets/d/1V6Pr6xWrHSHaothB61MJbR7mPFHwQ1u6/export?format=xlsx', engine='openpyxl')

goog_reviews.info()
goog_reviews.head()

tp_reviews.info()
tp_reviews.head()

"""#Conducting Initial Data Investigation"""

#Remove rows with missing values in Comment column (Google) and Review Content column (Trustpilot)
goog_reviews = goog_reviews[goog_reviews['Comment'].notnull()]
tp_reviews = tp_reviews[tp_reviews['Review Content'].notnull()]
#Check
print(goog_reviews['Comment'].isnull().sum())
print(tp_reviews['Review Content'].isnull().sum())

#Check for duplicated reviews
print(goog_reviews['Comment'].duplicated().sum())
print(tp_reviews['Review Content'].duplicated().sum())

#Remove duplicates, keeping only one of each duplicate
goog_reviews = goog_reviews.drop_duplicates(subset=['Comment'])
tp_reviews = tp_reviews.drop_duplicates(subset=['Review Content'])

print(goog_reviews['Comment'].duplicated().sum())
print(tp_reviews['Review Content'].duplicated().sum())

!pip install langdetect

#Filter to only english reviews
from langdetect import detect

def is_english(text):
    try:
        return detect(text) == "en"
    except:
        return False  # Handle cases where detection fails

# Apply filter
goog_reviews = goog_reviews[goog_reviews['Comment'].apply(is_english)]
tp_reviews = tp_reviews[tp_reviews['Review Content'].apply(is_english)]

#Number of Unique locations
goog_locations= print(goog_reviews['Club\'s Name'].nunique())
tp_locations = print(tp_reviews['Location Name'].nunique())

#Unique locations
goog_locations= print(goog_reviews['Club\'s Name'].unique())
tp_locations = print(tp_reviews['Location Name'].unique())

import numpy as np
goog_locations_list = goog_reviews['Club\'s Name'].astype(str)
tp_locations_list = tp_reviews['Location Name'].astype(str)
goog_locations_sorted = np.sort(goog_locations_list).tolist()
tp_locations_sorted = np.sort(tp_locations_list).tolist()
print(goog_locations_sorted)
print(tp_locations_sorted)

#Find number of common locations between two datasets
common_locations = set(goog_reviews['Club\'s Name']).intersection(set(tp_reviews['Location Name']))
print(len(common_locations))
print(common_locations)

#Perform preprocessing of the data – change to lower case, remove stopwords using NLTK, and remove numbers.
import contractions
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

def preprocess_text(text):
    #Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)

    #remove_numbers
    text = re.sub(r'\d+', '', text)

    #Remove Pure Gym references
    text = text.replace("gym", "").strip()  # Replace "gym" with empty string and remove extra spaces
    text = text.replace("pure", "").strip()  # Replace "pure" with empty string and remove extra spaces

    #Handle contractions before tokenization
    text = contractions.fix(text)

    #Tokenise the text.
    tokens = word_tokenize(text.lower())

    #Remove stop words.
    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]

    # Lemmatise the tokens.
    # lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

    #Stemmatise the tokens
    # stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]

    # Join the tokens back into a string.
    processed_text = ' '.join(filtered_tokens)

    return processed_text

#Apply the function to preprocess two datasets
goog_reviews['clean_comment'] = goog_reviews['Comment'].apply(preprocess_text)
tp_reviews['clean_comment'] = tp_reviews['Review Content'].apply(preprocess_text)

goog_reviews.head()

#Find the frequency distribution of the words from each data set's reviews separately. You can use nltk.freqDist.
from nltk.probability import FreqDist
goog_freq_dist = FreqDist(word_tokenize(' '.join(goog_reviews['clean_comment'])))
tp_freq_dist = FreqDist(word_tokenize(' '.join(tp_reviews['clean_comment'])))

#Plot a histogram/bar plot showing the top 10 words from each data set.
import matplotlib.pyplot as plt

# Get the top 10 most common words and their frequencies
top_10_goog = goog_freq_dist.most_common(10) # Use most_common() to get the top 10
goog_words, goog_freqs = zip(*top_10_goog) # Unzip into words and frequencies

# Plot
plt.figure(figsize=(10, 6))
plt.bar(goog_words, goog_freqs) # Use the extracted words and frequencies
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Top 10 Words in Google Reviews")
plt.xticks(rotation=45, ha='right')
plt.show()

# Get the top 10 most common words and their frequencies
top_10_tp = tp_freq_dist.most_common(10)
tp_words, tp_freqs = zip(*top_10_tp) # Unzip into words and frequencies

# Plot
plt.figure(figsize=(10, 6))
plt.bar(tp_words, tp_freqs) # Use the extracted words and frequencies
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Top 10 Words in Trustpilot Reviews")
plt.xticks(rotation=45, ha='right')
plt.show()

#Use the wordcloud library on the cleaned data and plot the word cloud.
from wordcloud import WordCloud

plt.figure(figsize = (100,100))
wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)
wc.generate(' '.join(goog_reviews['clean_comment']))
plt.imshow(wc)
plt.axis('off')

#Trustpilot Wordcloud
plt.figure(figsize = (100,100))
wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)
wc.generate(' '.join(tp_reviews['clean_comment']))
plt.imshow(wc)
plt.axis('off')

#"Create a new dataframe by filtering out the data to extract only the negative reviews from both data sets.
#   • For Google reviews, overall scores < 3 can be considered negative scores.
#   • For Trustpilot reviews, stars < 3 can be considered negative scores.

# Repeat the frequency distribution and wordcloud steps on the filtered data consisting of only negative reviews."

#GOOGLE
goog_negative_reviews = goog_reviews[goog_reviews['Overall Score'] < 3]
goog_freq_neg_dist = FreqDist(word_tokenize(' '.join(goog_negative_reviews['clean_comment'])))

# Get the top 10 most common negative words and their frequencies
top_10_neg_goog = goog_freq_neg_dist.most_common(10) # Use most_common() to get the top 10
goog_words, goog_freqs = zip(*top_10_neg_goog) # Unzip into words and frequencies

# Plot
plt.figure(figsize=(10, 6))
plt.bar(goog_words, goog_freqs) # Use the extracted words and frequencies
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Top 10 Words in Negative Google Reviews")
plt.xticks(rotation=45, ha='right')
plt.show()

#TRUSTPILOT
tp_negative_reviews = tp_reviews[tp_reviews['Review Stars'] < 3]
tp_freq_neg_dist = FreqDist(word_tokenize(' '.join(tp_negative_reviews['clean_comment'])))

# Get the top 10 most common words and their frequencies
top_10_neg_tp = tp_freq_neg_dist.most_common(10)
tp_words, tp_freqs = zip(*top_10_neg_tp) # Unzip into words and frequencies

# Plot
plt.figure(figsize=(10, 6))
plt.bar(tp_words, tp_freqs) # Use the extracted words and frequencies
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Top 10 Words in Negative Trustpilot Reviews")
plt.xticks(rotation=45, ha='right')
plt.show()

#Google Negative Wordcloud
plt.figure(figsize = (100,100))
wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)
wc.generate(' '.join(tp_negative_reviews['clean_comment']))
plt.imshow(wc)
plt.axis('off')

#Trustpilot Negative Wordcloud
plt.figure(figsize = (100,100))
wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)
wc.generate(' '.join(goog_negative_reviews['clean_comment']))
plt.imshow(wc)
plt.axis('off')

"""# Conducting Initial Topic Modelling"""

#"With the data frame created in the previous step:
    # • Filter out the reviews that are from the locations common to both data sets.
    # • Merge the reviews to form a new list."

goog_negative_reviews_both_locations = goog_negative_reviews[goog_negative_reviews['Club\'s Name'].isin(common_locations)]
tp_negative_reviews_both_locations = tp_negative_reviews[tp_negative_reviews['Location Name'].isin(common_locations)]

goog_negative_reviews_both_locations_list = goog_negative_reviews_both_locations['clean_comment'].tolist()
tp_negative_reviews_both_locations_list = tp_negative_reviews_both_locations['clean_comment'].tolist()

all_negative_reviews_both_locations_list = goog_negative_reviews_both_locations_list + tp_negative_reviews_both_locations_list
all_negative_reviews_both_locations_list[:5]

#Import necessary library components
from bertopic import BERTopic

model = BERTopic(verbose=True)
model.fit(all_negative_reviews_both_locations_list)
topic, probabilities = model.transform(all_negative_reviews_both_locations_list)

model.get_topic_freq().head(10)

model.get_topic(-1)

#Top topic
model.get_topic(0)

#Second top topic
model.get_topic(1)

model.visualize_topics()

"""Comment: Many overlapping clusters with few distinctly large cicrles makes it difficult to evaluate."""

model.visualize_barchart()

model.visualize_heatmap()

hierarchical_topics = model.hierarchical_topics(all_negative_reviews_both_locations_list)
model.visualize_hierarchy(hierarchical_topics=hierarchical_topics, top_n_topics=10)

""""For 10 clusters, provide a brief description in the Notebook of the topics they comprise of along with the general theme of the cluster, evidenced by the top words within each cluster's topics"


1. Car Parking: customers are having problems parking for the gym.
2. Pin code: customers are struggling with access to the gym.
3. Weights: issues with equipment type
4. Busy equipment: gym is too busy for some equipment.
5. Loud music/noise: gym environment is too loud.
6. Instructor classes: issues with instructor led classes.
7. Air conditioning issues: Ac not workign as expected.
8. Cold showers: temperature of the showers too cold.
9. Changing toilets: locker room & toilets are not as expected.
10. Locker locks: Personal items are not locked away as expected.

# Performing Further Data Investigation
"""

#List out the top 20 locations with the highest number of negative reviews.
#Do this separately for Google and Trustpilot's reviews, and comment on the result. Are the locations roughly similar in both data sets?
goog_negative_reviews_location_freq = goog_negative_reviews['Club\'s Name'].value_counts()
tp_negative_reviews_location_freq = tp_negative_reviews['Location Name'].value_counts()

print("Google Negative Reviews:")
print(goog_negative_reviews_location_freq.head(20))
print("\nTrustpilot Negative Reviews:")
print(tp_negative_reviews_location_freq.head(20))

goog_negative_reviews_location = goog_negative_reviews['Club\'s Name']
tp_negative_reviews_location = tp_negative_reviews['Location Name']
common_negative_locations = set(goog_negative_reviews_location).intersection(set(tp_negative_reviews_location))
# Create a dictionary to store combined frequencies for common locations
combined_frequencies = {}
for location in common_negative_locations:
    goog_freq = goog_negative_reviews_location_freq.get(location, 0)  # Get Google freq, default 0 if not found
    tp_freq = tp_negative_reviews_location_freq.get(location, 0)    # Get Trustpilot freq, default 0 if not found
    combined_frequencies[location] = goog_freq + tp_freq  # Combine frequencies

# Sort locations based on combined frequency (descending order)
sorted_locations = sorted(combined_frequencies.items(), key=lambda item: item[1], reverse=True)

# Get the top 20 locations
top_20_negative_locations = [location for location, freq in sorted_locations[:20]]

print("Top 20 Locations by Negative Reviews:")
for location in top_20_negative_locations:
    print(f"- {location}: {combined_frequencies[location]} reviews")

"""London dominates the list for poor reviews. it is possible that these could be the busiest gyms and that relative to total number of reviews, this is not a clear picture of poor performance."""

#Check % of negative reviews out of total reviews for each top 20 locations
total_negative_reviews_per_location = {}
for location in top_20_negative_locations:
       goog_total = goog_reviews[goog_reviews['Club\'s Name'] == location].shape[0]
       tp_total = tp_reviews[tp_reviews['Location Name'] == location].shape[0]
       total_negative_reviews_per_location[location] = goog_total + tp_total

negative_review_percentage = {}
for location in top_20_negative_locations:
       negative_count = combined_frequencies[location]
       total_count = total_negative_reviews_per_location[location]
       percentage = (negative_count / total_count) * 100
       negative_review_percentage[location] = percentage

   # Print the results
for location, percentage in negative_review_percentage.items():
       print(f"{location}: {percentage:.2f}% negative reviews")

"""Locations in top highest absolute number of reviews also have a large % of their total reviews (positive & negative) suggesting they have some problems to resolve specific to that gym."""

#"Merge the 2 data sets using Location Name and Club's Name.
def standardize_location(location):
    # Convert location to string if it's not already
    location = str(location)
    location = location.lower()  # Convert to lowercase
    return location

goog_negative_reviews['Club\'s Name'] = goog_negative_reviews['Club\'s Name'].apply(standardize_location)
tp_negative_reviews['Location Name'] = tp_negative_reviews['Location Name'].apply(standardize_location)

merged_negative_reviews_locations = pd.merge(goog_negative_reviews, tp_negative_reviews, left_on=['Club\'s Name'], right_on=['Location Name'], how='inner')
merged_negative_reviews_locations.head()

# Now, list out the following:
    # • Locations
    # • Number of Trustpilot reviews for this location
    # • Number of Google reviews for this location
    # • Total number of reviews for this location (sum of Google reviews and Trustpilot reviews)

# Group by standardized location and calculate Total Reviews
summary_merged_negative_reviews_locations = merged_negative_reviews_locations.groupby(['Club\'s Name']).size().reset_index(name='Total Reviews')  # Group by 'Club\'s Name'

# Calculate Google and Trustpilot reviews per location
goog_counts = goog_negative_reviews.groupby('Club\'s Name')['Club\'s Name'].count().rename('Google Reviews')
tp_counts = tp_negative_reviews.groupby('Location Name')['Location Name'].count().rename('Trustpilot Reviews')

# Rename the 'Location Name' column in tp_counts to 'Club's Name' for merging
tp_counts = tp_counts.rename_axis(index='Club\'s Name').reset_index()  # Reset index to create 'Club's Name' column

# Merge counts into summary table using 'Club's Name' as the merge key
summary_merged_negative_reviews_locations = summary_merged_negative_reviews_locations.merge(goog_counts, on='Club\'s Name', how='left').merge(tp_counts, on='Club\'s Name', how='left')

# Fill NaN values with 0 (for locations with no reviews from a specific source)
summary_merged_negative_reviews_locations[['Google Reviews', 'Trustpilot Reviews']] = summary_merged_negative_reviews_locations[['Google Reviews', 'Trustpilot Reviews']].fillna(0)

# Recalculate Total Reviews to ensure accuracy
summary_merged_negative_reviews_locations['Total Reviews'] = summary_merged_negative_reviews_locations['Google Reviews'] + summary_merged_negative_reviews_locations['Trustpilot Reviews']

# Display the updated table
summary_merged_negative_reviews_locations.sort_values(by='Total Reviews', ascending=False)

#For the top 30 locations, redo the word frequency and word cloud.
#Comment on the results, and highlight if the results are different from the first run.

top_30_locations = summary_merged_negative_reviews_locations.sort_values(by='Total Reviews', ascending=False).head(30)['Club\'s Name'].tolist()

# Filter original dataframes to include only top 30 locations
goog_top_30 = goog_negative_reviews[goog_negative_reviews['Club\'s Name'].isin(top_30_locations)]
tp_top_30 = tp_negative_reviews[tp_negative_reviews['Location Name'].isin(top_30_locations)]

# Combine 'clean_comment' from both dataframes
all_top_30_reviews = goog_top_30['clean_comment'].tolist() + tp_top_30['clean_comment'].tolist()

# Calculate word frequency
top_30_reviews_freq = FreqDist(word_tokenize(' '.join(all_top_30_reviews)))

# Get top 10 words
top_30_combined = top_30_reviews_freq.most_common(10)
words, freqs = zip(*top_30_combined)

# Plot
plt.figure(figsize=(10, 6))
plt.bar(words, freqs)  # Use extracted words and frequencies for top 30 locations
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.title("Top 10 Words in Negative Reviews for Top 30 Locations")
plt.xticks(rotation=45, ha='right')
plt.show()

#Google Negative Wordcloud for top 30 locations
plt.figure(figsize = (100,100))
wc = WordCloud(background_color = 'black', max_words = 1000,  max_font_size = 50)
wc.generate(' '.join(all_top_30_reviews)) # Use combined reviews for top 30 locations
plt.imshow(wc)
plt.axis('off')

#"For the top 30 locations, combine the reviews from Google and Trustpilot and run them through BERTopic.
model = BERTopic(verbose=True, low_memory=True)
model.fit(all_top_30_reviews)
topic, probabilities = model.transform(all_top_30_reviews)

## Comment on the following:
    # • Are the results any different from the first run of BERTopic?
    # • If so, what has changed?
    # • Are there any additional insights compared to the first run?"
model.get_topic_freq().head(10)

#Visualise top 30
model.visualize_topics()

model.visualize_barchart()

model.visualize_heatmap()

"""# Conducting Emotion Analysis"""

#Import the BERT model bhadresh-savani/bert-base-uncased-emotion from Hugging Face
!pip install datasets
from datasets import load_dataset
dataset = load_dataset("emotion")

#set up a pipeline for text classification.
from transformers import pipeline
emotion_classifier = pipeline(task="text-classification", model="bhadresh-savani/bert-base-uncased-emotion")

#With the help of an example sentence, run the model and display the different emotion classifications that the model outputs.
example_sentence = "I am feeling very happy today!"
result = emotion_classifier(example_sentence)
print(result)

example_sentence = "Great! the treadmill is broken again."
result = emotion_classifier(example_sentence)
print(result)

#Run this model on both data sets, and capture the top emotion for each review.
from transformers import AutoTokenizer

# Load the tokenizer for the emotion classification model
tokenizer = AutoTokenizer.from_pretrained("bhadresh-savani/bert-base-uncased-emotion")

# Function to truncate reviews to a maximum length using model's tokenizer
def truncate_text(text, max_length=510):
    inputs = tokenizer(text, truncation=True, max_length=max_length)
    return tokenizer.decode(inputs["input_ids"])

# Apply truncation to review data
goog_negative_reviews['truncated_comment'] = goog_negative_reviews['clean_comment'].apply(truncate_text)
tp_negative_reviews['truncated_comment'] = tp_negative_reviews['clean_comment'].apply(truncate_text)

# Use the truncated reviews for emotion classification
goog_classifier_results = emotion_classifier(goog_negative_reviews['truncated_comment'].tolist())
tp_classifier_results = emotion_classifier(tp_negative_reviews['truncated_comment'].tolist())

#Convert to dataframe
goog_classifier_df = pd.DataFrame(goog_classifier_results)
tp_classifier_df = pd.DataFrame(tp_classifier_results)

#add the top emotion as a column to your DataFrames
goog_negative_reviews['emotion'] = goog_classifier_df['label']  # Access the 'label' column directly
tp_negative_reviews['emotion'] = tp_classifier_df['label']  # Access the 'label' column directly

# Calculate emotion frequency and find highest emotion
goog_emotion_counts = goog_negative_reviews['emotion'].value_counts()
highest_goog_emotion = goog_emotion_counts.index[0]
print(f"Highest emotion in Google negative reviews: {highest_goog_emotion} ({goog_emotion_counts[highest_goog_emotion]} occurrences)")

tp_emotion_counts = tp_negative_reviews['emotion'].value_counts()
highest_tp_emotion = tp_emotion_counts.index[0]
print(f"Highest emotion in Trustpilot negative reviews: {highest_tp_emotion} ({tp_emotion_counts[highest_tp_emotion]} occurrences)")

#Use a bar plot to show the top emotion distribution for all negative reviews in both data sets.
import matplotlib.pyplot as plt
plt.figure
goog_emotion_counts.plot(kind='bar')
plt.xlabel('Emotion')
plt.ylabel('Frequency')
plt.title('Emotion Distribution in Google Negative Reviews')
plt.show

#tp Emotion Distribution
tp_emotion_counts.plot(kind='bar')
plt.xlabel('Emotion')
plt.ylabel('Frequency')
plt.title('Emotion Distribution in Trustpilot Negative Reviews')
plt.show

"""Even for sarcastic reviews, joy is high frequency in both data sets."""

#Extract all the negative reviews (from both data sets) where anger is top emotion.
goog_anger_reviews = goog_negative_reviews[goog_negative_reviews['emotion'] == 'anger']
tp_anger_reviews = tp_negative_reviews[tp_negative_reviews['emotion'] == 'anger']

goog_anger_reviews.info() #Check data

tp_anger_reviews.info() #Check data

# Combine anger comments from both sources
#Select the 'clean_comment' column from both DataFrames and convert them to lists
all_angry_reviews = goog_anger_reviews['clean_comment'].tolist() + tp_anger_reviews['clean_comment'].tolist()

# Initialize and fit the BERTopic model
model = BERTopic(verbose=True, nr_topics=10)
topics, probabilities = model.fit_transform(all_angry_reviews)

all_angry_reviews[0:10]

#Visualise the clusters from this run. Comment on whether it is any different from the previous runs, and whether it is possible to narrow down the primary issues that have led to an angry review.
model.get_topic_freq().head(10)

model.get_topic_info()

# Get a list of topics, excluding the outlier topic -1
topics_to_visualize = [topic for topic in model.get_topic_freq().Topic.tolist() if topic != -1]
model.visualize_heatmap(topics=topics_to_visualize, title='Angry Reviews Similarity Matrix')

model.visualize_barchart(topics=topics_to_visualize, title='Angry Reviews Topic Word Scores')

hierarchical_topics = model.hierarchical_topics(docs=all_angry_reviews)
model.visualize_hierarchy(hierarchical_topics=hierarchical_topics[:10])

"""# Using a large language model from Hugging Face"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import numpy as np
import json
import re

all_negative_reviews = goog_negative_reviews['clean_comment'].tolist() + tp_negative_reviews['clean_comment'].tolist()

# Load the model
model_phi = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")

pipe = pipeline("text-generation", model=model_phi, tokenizer=tokenizer)

generation_args = {
    "return_full_text": False,
    "temperature": 0.0,
    "do_sample": False,
    "max_length": 1000,
    "truncation": True
}

all_negative_reviews[0:3]

goog_negative_reviews[['clean_comment','truncated_comment']]

# "Add the following prompt to every review, before passing it on to the model: In the following customer review, pick out the main 3 topics.
#Return them in a numbered list format, with each one on a new line.
# Run the model.
# Note: If the execution time is too high, you can use a subset of the bad reviews (instead of the full set) to run this model."
topics_llm = []
print(len(all_negative_reviews))
iter = 0
for index, review in enumerate(all_negative_reviews[0:50]):
  if len(str(review)) < 1000:
    #prompt the model with a direct clear message.
    message_1 = (
    {"Extract EXACTLY three main topics from the following customer review. "},
    {"Return ONLY a valid JSON list of strings, with NO extra text, headers, or explanations."},
    {"Example output format: [\"topic1\", \"topic2\", \"topic3\"]"}, {review},
)

    prompt = f"{message_1} {review}"
    output = pipe(prompt, **generation_args)

    generated_text = output[0]['generated_text']

    # Extract JSON-like list from output
    json_pattern = r'\[.*?\]'
    match = re.search(json_pattern, generated_text, re.DOTALL)
    if match:
        topic_list_string = match.group(0)
    else:
        print(f"Invalid JSON format: {generated_text}")
        topic_list_string = "[]"

    # Convert to JSON
    try:
        topic_list = json.loads(topic_list_string)
        topics_llm.append(topic_list)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON at index {index}: {e}, Raw output: {generated_text}")
    topics_llm.append(topic_list)

"""The model performs overall well on the 50 reviews sampled. However, there are instances where the prompt is not followed properly and/or there is an error in the output."""

# Convert the list of lists to a list of strings before creating the NumPy array
topic_string_array = [item for row in topics_llm for item in row]
np_topics_llm = np.array(topic_string_array) # Create the NumPy array from the list of strings
np.save('np_topics_phi.npy', np_topics_llm)

#The output of the model will be the top 3 topics from each review. Append each of these topics from each review to create a comprehensive list.
import numpy as np
loaded_arr = np.load('np_topics_phi.npy')
topics_llm = loaded_arr.tolist()
print(topics_llm)

#Use this list as input to run BERTopic again.
#Flatten the list of lists into a single list of strings
# flattened_topics_llm = [topic for row in topics_llm for topic in row]

model = BERTopic(verbose=True)
model.fit(topics_llm) # Pass the flattened list to fit()
topic, probabilities = model.transform(topics_llm) # Pass the flattened list to transform() as well

#Comment about the output of BERTopic. Highlight any changes, improvements, and if any further insights have been obtained.
model.get_topic_freq().head(10)

#List the topics from above
model.visualize_topics()

model.get_topic_info()

#"Use the comprehensive list.
#Pass it to the model as the input, but pre-fix the following to the prompt:
#For the following text topics obtained from negative customer reviews, can you give some actionable insights that would help this gym company?
for text in topic_string_array:
    message_2 = (
    {"Based on the following list of customer feedback topics related to gym facilities, offerings, and customer service, provide 10 specific and actionable insights to help this gym company improve customer experience."},
    {"Focus on areas such as equipment, cleanliness, class offerings, customer service, and overall gym environment. Avoid general statements or suggestions related to employee hiring or training."},
    {"Return the insights in a numbered list format, with each insight on a new line."},
    {"Example format:"},
    {"1. Ensure all equipment is functional and well-maintained, addressing any reported issues promptly."},
    {"2. Implement a rigorous cleaning schedule to maintain high standards of hygiene in all areas of the gym, including restrooms and locker rooms."},
    {"..."},
    {"10.  ..."},
    {"Topics:"}, {text}
)

    prompt = f"{message_2}"
    output_2 = pipe(prompt, **generation_args)

#List the output, ideally in the form of suggestions, that the company can employ to address customer concerns.
insights = output_2[0]['generated_text']
print(insights)

"""Your list of customer feedback topics related to gym facilities, offerings, and customer service might look something like this:

1. Equipment availability and maintenance
2. Cleanliness and hygiene standards
3. Class offerings and variety
4. Customer service and staff responsiveness
5. Gym environment and ambiance
6. Pricing and membership options
7. Accessibility and inclusivity
8. Personal training and support services
9. Technology and app integration
10. Community events and social atmosphere

Based on these topics, here are 10 specific and actionable insights to help the gym company improve customer experience:

1. Conduct regular equipment checks and maintenance to ensure all machines are in working order and safe for use.
2. Increase the frequency of cleaning, especially in high-traffic areas, and provide clear signage indicating when the gym is being cleaned.
3. Expand the range of classes offered, including beginner-friendly options and specialized classes for different fitness levels and interests.
4. Train staff to be more proactive in assisting members, including offering personalized recommendations and addressing concerns promptly.
5. Redesign the gym layout to create a more welcoming and comfortable environment, with better lighting and more open spaces.
6. Review and adjust membership pricing to offer more flexible options, such as pay-per-class or family packages.
7. Ensure the gym is fully accessible to individuals with disabilities, with appropriate equipment and facilities.
8. Offer a variety of personal training sessions, including virtual options, to cater to members' different schedules and preferences.
9. Integrate the gym's app with real-time updates on class schedules, equipment availability, and member progress tracking.
10. Host regular community events, such as fitness challenges or social gatherings, to foster a sense of community and encourage member engagement.

These insights are directly related to the topics of gym facilities, offerings, and customer service, and they provide specific, actionable steps that the gym company can take to enhance the overall customer experience.

#Using Gensim
"""

!pip install --upgrade --force-reinstall numpy
!pip install --upgrade --force-reinstall scipy
!pip install --upgrade --force-reinstall gensim

# !pip install --upgrade numpy
# !pip install --upgrade scipy
# !pip install --upgrade gensim

#import necessary libraries
from gensim import corpora
from gensim.models import LdaModel
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

#Perform the preprocessing required to run the LDA model from Gensim. Use the list of negative reviews (combined Google and Trustpilot reviews).

# Define stop words and punctuation.
stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()

# Function to clean the document.
def clean(doc):
    # Remove stop words and convert to lowercase.
    stop_free = " ".join([word for word in doc.lower().split() if word not in stop])
    # Remove punctuation.
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    # Lemmatise the text.
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized

# all_negative_reviews = goog_negative_reviews['clean_comment'].tolist() + tp_negative_reviews['clean_comment'].tolist()
all_negative_reviews_clean = [clean(doc).split() for doc in all_negative_reviews]

#Using Gensim, perform LDA on the tokenised data. Specify the number of topics = 10.
from gensim import corpora
dictionary = corpora.Dictionary(all_negative_reviews_clean)
corpus = [dictionary.doc2bow(text) for text in all_negative_reviews_clean]

num_topics = 10
passes = 20
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, random_state=42)

# Print the topics.
for idx, topic in lda_model.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic))

!pip install pyLDAvis

#Show the visualisations of the topics, displaying the distance maps and the bar chart listing out the most salient terms.
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Prepare the visualisation.
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)

"""Comment on the output and whether it is similar to other techniques, and whether any extra insights were obtained.

There are common themes here primarily on equipment quality & availability, staff professionalism with more minor topics such as showers, gym access and parking.
"""

import numpy as np

# Save important variables
np.save('my_notebook_variables.npy', {
    'goog_reviews': goog_reviews,
    'tp_reviews': tp_reviews,
    'goog_negative_reviews': goog_negative_reviews,
    'tp_negative_reviews': tp_negative_reviews,
    'all_negative_reviews_both_locations_list': all_negative_reviews_both_locations_list,
    'topics_llm': topics_llm,  # Add other important variables here as needed
})

# Load variables (run after restarting Colab)
import numpy as np
loaded_variables = np.load('my_notebook_variables.npy', allow_pickle=True).item()
goog_reviews = loaded_variables['goog_reviews']
tp_reviews = loaded_variables['tp_reviews']
goog_negative_reviews = loaded_variables['goog_negative_reviews']
tp_negative_reviews = loaded_variables['tp_negative_reviews']
all_negative_reviews_both_locations_list = loaded_variables['all_negative_reviews_both_locations_list']
topics_llm = loaded_variables['topics_llm']  # Load other important variables